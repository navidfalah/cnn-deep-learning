# -*- coding: utf-8 -*-
"""cnn_deeplearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1enepqRhuyEtToJGy4eRlBu89UmQmxBvt
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim

import matplotlib.pyplot as plt
import numpy as np

import torch

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

### loading the datasets and make it in a dataloader

transform_train = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

batch_size = 64

trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

# Assuming the dataset sizes and batch size
total_train_data = len(trainloader.dataset)  # This gives the total number of training images
total_test_data = len(testloader.dataset)

print(f"Total training data: {total_train_data}")
print(f"Total testing data: {total_test_data}")

### alex net architectures

class AlexNet(nn.Module):
    def __init__(self, num_classes=100):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
alexnet = AlexNet().cuda()

vgg16 = torchvision.models.vgg16(pretrained=True)
vgg16.classifier[6] = nn.Linear(4096, 100)
vgg16.cuda()

resnet50 = torchvision.models.resnet50(pretrained=True)
resnet50.fc = nn.Linear(resnet50.fc.in_features, 100)
resnet50.cuda()

googlenet = torchvision.models.googlenet(pretrained=True)
googlenet.fc = nn.Linear(googlenet.fc.in_features, 100)
googlenet.cuda()

def train_model(model, criterion, optimizer, trainloader, epochs=10):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(trainloader):
            inputs, labels = inputs.cuda(), labels.cuda()

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 100 == 99:
                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100:.3f}')
                running_loss = 0.0
    print('Finished Training')

criterion = nn.CrossEntropyLoss()

# For AlexNet
optimizer_alex = optim.SGD(alexnet.parameters(), lr=0.01, momentum=0.9)
train_model(alexnet, criterion, optimizer_alex, trainloader)

# For VGG16
optimizer_vgg = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)
train_model(vgg16, criterion, optimizer_vgg, trainloader)

# For ResNet50
optimizer_resnet = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9)
train_model(resnet50, criterion, optimizer_resnet, trainloader)

# For GoogleNet
optimizer_google = optim.SGD(googlenet.parameters(), lr=0.001, momentum=0.9)
train_model(googlenet, criterion, optimizer_google, trainloader)

def evaluate_model(model, testloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.cuda(), labels.cuda()
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f'Accuracy of the network on the test images: {accuracy} %')
    return accuracy

evaluate_model(alexnet, testloader)
evaluate_model(vgg16, testloader)
evaluate_model(resnet50, testloader)
evaluate_model(googlenet, testloader)

torch.save(alexnet.state_dict(), 'alexnet_cifar100.pth')
torch.save(vgg16.state_dict(), 'vgg16_cifar100.pth')
torch.save(resnet50.state_dict(), 'resnet50_cifar100.pth')
torch.save(googlenet.state_dict(), 'googlenet_cifar100.pth')

### new part from the book

from torchvision import models

transfer_model = models.resnet50(pretrained=True)

for name, param in transfer_model.named_parameters():
    print(f'Parameter Name: {name:30} | Shape: {str(param.shape):30} | Trainable: {param.requires_grad} | Number of Elements: {param.numel()}')

for name, param in transfer_model.named_parameters():
 if("bn" not in name):
  param.requires_grad = False

for name, param in transfer_model.named_parameters():
    print(f'Parameter Name: {name:30} | Shape: {str(param.shape):30} | Trainable: {param.requires_grad} | Number of Elements: {param.numel()}')

transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features, 500),
                                  nn.ReLU(),
                                  nn.Dropout(),
                                  nn.Linear(500, 2))

import torch.optim as optimizer

optimizer = optimizer.Adam([
{ 'params': transfer_model.layer4.parameters(), 'lr': found_lr /3},
{ 'params': transfer_model.layer3.parameters(), 'lr': found_lr /9},
], lr=found_lr)

